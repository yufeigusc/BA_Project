---
title: "R Notebook Template for Competition"
author: ''
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    number_sections: true
    theme: readable
    toc: true
  pdf_document:
    toc: true
---

```{r global-options, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      fig.height = 6,
                      fig.width = 6,
                      fig.align = "center")
```
```{r}
options(warn=-1)
```

# Loading and Exploring the Data Set


Read the data files as usual.
```{r}
train = read.csv("Competition_Train.csv")
test = read.csv("Competition_Test.csv")
str(train)
str(test)
```
```{r}
new_risk_flags_sum <- rowSums(train[, c("deck_risk", "oe_constraint", "ppap_risk", "stop_auto_buy", "rev_stop")])
```

We can convert categorical variables to appropriate types.
```{r}
train$potential_issue = as.factor(train$potential_issue)
train[18:23] = lapply(train[18:23], as.factor)
str(train)
test$potential_issue = as.factor(test$potential_issue)
test[18:22] = lapply(test[18:22], as.factor)
str(test)
```


You are encouraged to explore the data set in more detail to understand the variables and obtain some insights that might help you build better models.


```{r}
#cor(idvar)
```


Later, we are going to use two new packages, "caret" and "e1071", to carry out the cross-validation for a logistic regression model.  Before that, we need to change the level labels since we want to use AUC as the performance measure in cross-validation. You can test the cross-validation without running the following code chunk, and see what error message you get.
```{r}
library(plyr)
train$potential_issue = revalue(train$potential_issue, c("0"="No", "1"="Yes"))
train[18:23] = lapply(train[18:23], function(x) revalue(x, c("0"="No", "1"="Yes")))
str(train)
test$potential_issue = revalue(test$potential_issue, c("0"="No", "1"="Yes"))
test[18:22] = lapply(test[18:22], function(x) revalue(x, c("0"="No", "1"="Yes")))
str(test)
```


# Comparing Two Prediction Models


In this demonstration, we will only compare two logistic regression models, one using all the independent variables in the data set and the other using only one independent variable, *national_inv*. We will use $k$-fold cross-validation to compare the performance of these two models. Let's install and load two new packages.
```{r echo = TRUE, message = FALSE, warning = FALSE}
library(caret)
library(e1071)
```
```{r}
model1=glm(went_on_backorder ~ . - sku,data=train,family=binomial)
summary(model1)
```

First, we will define our cross-validation experiment.
```{r}
fitControl = trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
```
The first argument, *method = "cv"*, tells the function to use the cross-validation method. The additional arguments are necessary since we want to use AUC as the performance measure in cross-validation.


Now, we are ready to perform cross-validation. Since there is some random component in $k$-fold cross-validation---random partition into $k$ folds---we can synchronize the results by setting a common random seed first. You can explore different seeds in the competition. We will use $k$-fold cross-validation to obtain the first model's performance. Recall that our first model is a logistic regression model using all the independent variables. There are too many variables in the data set, so we will use a trick to make the coding easier. The dot '.' used on the right-hand side of the prediction equation below indicates that all the variables in the data frame except the dependent variable are used in the prediction. However, we do not want to include *sku* in the model, so we add the argument *- sku*.
```{r}
set.seed(123)
train(went_on_backorder ~ . - sku, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```


When you run the above code chunk, there may be a warning saying "glm.fit: fitted probabilities numerically 0 or 1 occurred." The message indicates that some predicted numbers are too close to 0 or 1. This can be caused by extreme values (outliers) in some variables. In the demonstration, we ignore the warning message and proceed to use the model and its prediction. However, this suggests that you should explore the data more carefully and understand the data better so that you may find ways to build better models. Note that the warning message may repeat $k$ times due to $k$-fold cross-validation.



The reported sensitivity (*Sens*) and specificity (*Spec*) are computed using the threshold value of 0.5, which you can ignore in this case since the performance measure for the competition is AUC, reported under *ROC* in the above output.


The second model's cross-validation performance can be obtained in a similar way. Note that you may want to use the same seed here to ensure that the difference in cross-validation performance is not due to the $k$-fold random partition.
```{r}
set.seed(123)
train(went_on_backorder ~ national_inv, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```


Between these two logistic regression models, the first one with all the independent variables performs better in cross-validation. Now, we will retrain the selected model using the whole training set.
```{r}
modelBO = glm(went_on_backorder ~ . - sku, data = train, family = "binomial")
summary(modelBO)
```
Remove variable in 0.1 significant level.
```{r}
set.seed(123)
train(went_on_backorder ~ . - sku- rev_stop-forecast_9_month, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```

Remove variable with 0.05 significant level.
```{r}
set.seed(123)

train(went_on_backorder ~ . - sku- rev_stop-forecast_9_month-stop_auto_buy-sales_1_month-forecast_3_month, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```


#create some variables
```{r}
#Transit Efficiency ROC:0.82455
modelBO = glm(went_on_backorder ~ . - sku+lead_time/in_transit_qty, data = train, family = "binomial")
summary(modelBO)
set.seed(123)
train(went_on_backorder ~ . - sku+lead_time/in_transit_qty, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")

```

```{r}
# Performance Change-> Do Not Use ROC:0.8308114
train$new_perf_change <- train$perf_12_month_avg - train$perf_6_month_avg

modelBO = glm(went_on_backorder ~  . - sku-perf_12_month_avg-perf_6_month_avg, data = train, family = "binomial")
summary(modelBO)

set.seed(123)
train(went_on_backorder ~  . - sku-perf_12_month_avg-perf_6_month_avg, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```

```{r}
#use perf change and transit efficiency ROC:  0.8311262
set.seed(123)
train(went_on_backorder ~  . - sku-perf_12_month_avg-perf_6_month_avg+(lead_time/in_transit_qty), data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```

```{r}
modelBO = glm(went_on_backorder  ~  . - sku-perf_12_month_avg-perf_6_month_avg+(lead_time/in_transit_qty), data = train, family = "binomial")
summary(modelBO)

set.seed(123)
train(went_on_backorder ~  . - sku-perf_12_month_avg-perf_6_month_avg+(lead_time/in_transit_qty)-rev_stop-forecast_3_month-forecast_9_month-sales_1_month, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```

```{r}
# Risk Flags Sum-> Significant
train$new_risk_flags_sum <- new_risk_flags_sum

# ADD Risk Flags Sum without moving anything ROC:0.8311262
set.seed(123)
train(went_on_backorder ~  . - sku-perf_12_month_avg-perf_6_month_avg+lead_time/in_transit_qty, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")

#Remove all risk ROC:0.8332907
set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg , data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")

```





```{r}
# Remove lead time: best:ROC:0.8645098

set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
#if remove in_transit_qty, ROC become 0.8616967
set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-in_transit_qty, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```


```{r}
#only keep forecast_3_month ROC:0.8643431
#only keep forecast_6_month ROC:0.8643762
#only keep forecast_9_month ROC:0.8640531
#keep forecast_3_month and forecast_6_month ROC:0.8644153
#keep forecast_3_month and forecast_9_month ROC:0.8643166
#keep forecast_9_month and forecast_6_month ROC:0.864353
set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-forecast_3_month, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```

```{r}
# Calculate inventory turnover ratio with zero denominator handling
train$inventory_turnover_ratio <- ifelse((train$national_inv + train$in_transit_qty) == 0, 0, train$sales_9_month / ((train$national_inv + train$in_transit_qty) / 2))

#Add inventory turnover ratio ROC:0.8687388
#Remove in_transit_qty ROC: 0.8673415
#BEST: Remove sales_9_month ROC: 0.8693626
#Remove in_transit_qty and sales_9_month ROC: 0.8679585
#Remove in_transit_qty, national_inv and sales_9_month ROC:0.7617346
set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-sales_9_month, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```

```{r}
model1=glm(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-sales_9_month-new_perf_change,data=train,family=binomial)
summary(model1)
```

```{r}
# BEST:Romove new_perf_change, ROC:0.8699866
# Remove sales_1_month and new_perf_change, ROC:0.8693716
# *Prefer:Remove forecast_3_month, forecast_9_month, sales_1_month and new_perf_change, ROC:0.8693973
set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-sales_9_month-new_perf_change-forecast_3_month-forecast_9_month-sales_1_month , data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```
```{r}
# Calculate forecast accuracy with zero sales handling->Not Significant
#ROC not change

# train$forecast_accuracy_3_month <- ifelse(train$sales_3_month == 0, 0, 1 - abs(train$forecast_3_month - train$sales_3_month) / train$sales_3_month)
# 
# model1=glm(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-sales_9_month-new_perf_change,data=train,family=binomial)
# summary(model1)
# 
# set.seed(123)
# train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-sales_9_month-new_perf_change-forecast_3_month-forecast_9_month-sales_1_month , data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
#train <- subset(train , select =-forecast_accuracy_3_month)
```
```{r}
#remove min bank ROC=0.8698363
#continue move sales 6 months, ROC:0.8694322
#keep sales 1 month ROC: 0.8704557
set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-sales_9_month-new_perf_change-forecast_3_month-forecast_9_month-min_bank-sales_6_month, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")

```

```{r}
#remove min bank ROC=0.8698363
#continue move sales 6 months, ROC:0.8694322
#keep sales 1 month ROC: 0.8704557
set.seed(123)
train(went_on_backorder ~ . - sku-rev_stop-deck_risk-oe_constraint-ppap_risk- stop_auto_buy+lead_time/in_transit_qty-perf_12_month_avg-perf_6_month_avg-lead_time-sales_9_month-new_perf_change-forecast_3_month-forecast_9_month-min_bank-sales_6_month, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")

```



# Preparing the Predictions for Testing


Using the selected model, we make the predictions on the test set and save *sku* and the predicted probabilities of backorder in a data frame.
```{r}
PredBO = predict(modelBO, newdata = test, type = "response")
PredTest = data.frame(test$sku, PredBO)
str(PredTest)
```


```{r}
summary(PredTest)
```


Finally, we tally the variable names and save the predictions in a file. The file can be submitted. The argument, *row.names = FALSE*, prevent R from saving additional column with indecies for each row.
```{r}
colnames(PredTest) = c("sku", "went_on_backorder")
str(PredTest)
#write.csv(PredTest, "Sample_Submission.csv", row.names = FALSE)
```


