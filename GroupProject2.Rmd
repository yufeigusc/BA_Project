---
title: "R Notebook Template for Competition"
author: "Guo Yufei"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    number_sections: yes
    theme: readable
    toc: yes
---

```{r global-options, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      fig.height = 6,
                      fig.width = 6,
                      fig.align = "center")
```


# Loading and Exploring the Data Set


Read the data files as usual.
```{r}
train = read.csv("Competition_Train.csv")
test = read.csv("Competition_Test.csv")
#str(train)
#str(test)
```


We can convert categorical variables to appropriate types.
```{r}
train$potential_issue = as.factor(train$potential_issue)
train[18:23] = lapply(train[18:23], as.factor)
#str(train)
test$potential_issue = as.factor(test$potential_issue)
test[18:22] = lapply(test[18:22], as.factor)
#str(test)

```


You are encouraged to explore the data set in more detail to understand the variables and obtain some insights that might help you build better models.


Later, we are going to use two new packages, "caret" and "e1071", to carry out the cross-validation for a logistic regression model.  Before that, we need to change the level labels since we want to use AUC as the performance measure in cross-validation. You can test the cross-validation without running the following code chunk, and see what error message you get.
```{r}
library(plyr)
train$potential_issue = revalue(train$potential_issue, c("0"="No", "1"="Yes"))
train[18:23] = lapply(train[18:23], function(x) revalue(x, c("0"="No", "1"="Yes")))
#str(train)
test$potential_issue = revalue(test$potential_issue, c("0"="No", "1"="Yes"))
test[18:22] = lapply(test[18:22], function(x) revalue(x, c("0"="No", "1"="Yes")))
#str(test)
```


# Comparing Two Prediction Models


In this demonstration, we will only compare two logistic regression models, one using all the independent variables in the data set and the other using only one independent variable, *national_inv*. We will use $k$-fold cross-validation to compare the performance of these two models. Let's install and load two new packages.
```{r echo = TRUE, message = FALSE, warning = FALSE}
library(caret)
library(e1071)
```


First, we will define our cross-validation experiment.
```{r}
fitControl = trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
```
The first argument, *method = "cv"*, tells the function to use the cross-validation method. The additional arguments are necessary since we want to use AUC as the performance measure in cross-validation.


Now, we are ready to perform cross-validation. Since there is some random component in $k$-fold cross-validation---random partition into $k$ folds---we can synchronize the results by setting a common random seed first. You can explore different seeds in the competition. We will use $k$-fold cross-validation to obtain the first model's performance. Recall that our first model is a logistic regression model using all the independent variables. There are too many variables in the data set, so we will use a trick to make the coding easier. The dot '.' used on the right-hand side of the prediction equation below indicates that all the variables in the data frame except the dependent variable are used in the prediction. However, we do not want to include *sku* in the model, so we add the argument *- sku*.
```{r}
set.seed(123)
#train(went_on_backorder ~ . - sku, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```


When you run the above code chunk, there may be a warning saying "glm.fit: fitted probabilities numerically 0 or 1 occurred." The message indicates that some predicted numbers are too close to 0 or 1. This can be caused by extreme values (outliers) in some variables. In the demonstration, we ignore the warning message and proceed to use the model and its prediction. However, this suggests that you should explore the data more carefully and understand the data better so that you may find ways to build better models. Note that the warning message may repeat $k$ times due to $k$-fold cross-validation.



The reported sensitivity (*Sens*) and specificity (*Spec*) are computed using the threshold value of 0.5, which you can ignore in this case since the performance measure for the competition is AUC, reported under *ROC* in the above output.


The second model's cross-validation performance can be obtained in a similar way. Note that you may want to use the same seed here to ensure that the difference in cross-validation performance is not due to the $k$-fold random partition.
```{r}
set.seed(123)
#train(went_on_backorder ~ national_inv, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```


- Check correlation


```{r}
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)  
#str(temp_train)
#cor(temp_train)
```

- RFE feature elimination

- install
install.packages("randomForest")
install.packages("mlbench")


```{r}
# Load required libraries
library(caret)

library(mlbench)

# Load your dataset
data <- train

# Create a control function for RFE
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)

# Perform recursive feature elimination (RFE)
result <- rfe(data[, -which(names(data) == "went_on_backorder")], 
               data$went_on_backorder, 
               sizes = c(1:ncol(data)-1), 
               rfeControl = control)

# Print the results
print(result)

```


- LASSO regression model
- install
install.packages("glmnet")

```{r}
library(glmnet)

data <- read.csv("Competition_Train.csv")

# Define predictors (all variables except went_on_backorder)
predictors <- data[, !names(data) %in% "went_on_backorder"]
# Define response variable
response <- data$went_on_backorder

# Fit LASSO regression model
lasso_model <- glmnet(predictors, response, alpha = 1)

# Set the size of the plotting device
options(repr.plot.width=20, repr.plot.height=12)  # Adjust width and height as needed

# Plot coefficients
plot(lasso_model, xvar = "lambda", label = TRUE, col = 1:ncol(predictors))

# Add legend
legend("topright", legend = colnames(coef(lasso_model)), col = 1:ncol(predictors), lty = 1)


# Extract coefficients for all values of lambda
#lasso_coefficients <- coef(lasso_model)


```



```{r}
set.seed(123)
train(went_on_backorder ~ national_inv, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
```


Between these two logistic regression models, the first one with all the independent variables performs better in cross-validation. Now, we will retrain the selected model using the whole training set.
```{r}
modelBO = glm(went_on_backorder ~ . - sku, data = train, family = "binomial")
summary(modelBO)
```


# Preparing the Predictions for Testing


Using the selected model, we make the predictions on the test set and save *sku* and the predicted probabilities of backorder in a data frame.
```{r}
PredBO = predict(modelBO, newdata = test, type = "response")
PredTest = data.frame(test$sku, PredBO)
str(PredTest)
```


```{r}
summary(PredTest)
```


Finally, we tally the variable names and save the predictions in a file. The file can be submitted. The argument, *row.names = FALSE*, prevent R from saving additional column with indecies for each row.
```{r}
colnames(PredTest) = c("sku", "went_on_backorder")
str(PredTest)
write.csv(PredTest, "Sample_Submission.csv", row.names = FALSE)
```


