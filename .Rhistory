baseline_predictions = rep(baseline_prediction, nrow(dataTest))
baseline_accuracy = mean(baseline_predictions == dataTest$Mortality) * 100
baseline_accuracy = round(baseline_accuracy, 2)
str(baseline_accuracy)
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
res3b
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
#res3b
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
str(res3b)
set.seed(100)
predictTest = predict(model3a, newdata = dataTest, type = "response")
binary_predictions = ifelse(predictTest >= 0.1, 1, 0)
accuracy = mean(binary_predictions == dataTest$Mortality) * 100
accuracy = round(accuracy, 2)
str(accuracy)
knitr::opts_chunk$set(warning = FALSE,
fig.height = 6,
fig.width = 6,
fig.align = "center")
data = read.csv("Assignment_Logistic_Regression_Data.csv")
#str(data)
library(caTools)
set.seed(100)
set.seed(100)
split = sample.split(data$Mortality, SplitRatio = 0.7)
dataTrain = subset(data, split == TRUE)
dataTest = subset(data, split == FALSE)
train_observation_num = nrow(dataTrain)
str(train_observation_num)
mort_rate = sum(dataTrain$Mortality) / train_observation_num
str(mort_rate)
set.seed(100)
model2a = glm(Mortality ~ . , data = dataTrain, family = binomial)
summary(model2a)
set.seed(100)
model2c = glm(Mortality ~ SystolicBP + SpO2 + Haemoglobin + Urea , data = dataTrain, family = binomial)
summary(model2c)
set.seed(100)
model3a = glm(Mortality ~ SystolicBP + SpO2 + Haemoglobin + Urea , data = dataTrain, family = binomial)
predictTrain = predict(model3a, type = "response")
cor(dataTrain[, c("SystolicBP", "SpO2", "Haemoglobin", "Urea")])
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
str(res3b)
library(ROCR)
set.seed(100)
predictTest = predict(model3a, newdata = dataTest, type = "response")
binary_predictions = ifelse(predictTest >= 0.1, 1, 0)
accuracy = mean(binary_predictions == dataTest$Mortality) * 100
accuracy = round(accuracy, 2)
str(accuracy)
cof1 = table(dataTest$Mortality, predictTest >0.1 )
cof3 = table(dataTest$Mortality, predictTest >0.3 )
cof5 = table(dataTest$Mortality, predictTest >0.5 )
cof7 = table(dataTest$Mortality, predictTest >0.7 )
cof1
cof3
cof5
cof7
set.seed(100)
baseline_prediction = ifelse(mean(dataTrain$Mortality) > 0.5, 1, 0)
baseline_predictions = rep(baseline_prediction, nrow(dataTest))
baseline_accuracy = mean(baseline_predictions == dataTest$Mortality) * 100
baseline_accuracy = round(baseline_accuracy, 2)
str(baseline_accuracy)
data = read.csv("Assignment_Logistic_Regression_Data.csv")
#str(data)
library(caTools)
set.seed(100)
set.seed(100)
split = sample.split(data$Mortality, SplitRatio = 0.7)
dataTrain = subset(data, split == TRUE)
dataTest = subset(data, split == FALSE)
train_observation_num = nrow(dataTrain)
str(train_observation_num)
mort_rate = sum(dataTrain$Mortality) / train_observation_num
str(mort_rate)
set.seed(100)
model2a = glm(Mortality ~ . , data = dataTrain, family = binomial)
summary(model2a)
knitr::opts_chunk$set(warning = FALSE,
fig.height = 6,
fig.width = 6,
fig.align = "center")
data = read.csv("Assignment_Logistic_Regression_Data.csv")
#str(data)
library(caTools)
set.seed(100)
set.seed(100)
split = sample.split(data$Mortality, SplitRatio = 0.7)
dataTrain = subset(data, split == TRUE)
dataTest = subset(data, split == FALSE)
train_observation_num = nrow(dataTrain)
str(train_observation_num)
mort_rate = sum(dataTrain$Mortality) / train_observation_num
str(mort_rate)
set.seed(100)
model2a = glm(Mortality ~ . , data = dataTrain, family = binomial)
summary(model2a)
set.seed(100)
model2c = glm(Mortality ~ SystolicBP + SpO2 + Haemoglobin + Urea , data = dataTrain, family = binomial)
summary(model2c)
set.seed(100)
model3a = glm(Mortality ~ SystolicBP + SpO2 + Haemoglobin + Urea , data = dataTrain, family = binomial)
predictTrain = predict(model3a, type = "response")
cor(dataTrain[, c("SystolicBP", "SpO2", "Haemoglobin", "Urea")])
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
str(res3b)
library(ROCR)
set.seed(100)
predictTest = predict(model3a, newdata = dataTest, type = "response")
binary_predictions = ifelse(predictTest >= 0.1, 1, 0)
accuracy = mean(binary_predictions == dataTest$Mortality) * 100
accuracy = round(accuracy, 2)
str(accuracy)
cof1 = table(dataTest$Mortality, predictTest >0.1 )
cof3 = table(dataTest$Mortality, predictTest >0.3 )
cof5 = table(dataTest$Mortality, predictTest >0.5 )
cof7 = table(dataTest$Mortality, predictTest >0.7 )
cof1
cof3
cof5
cof7
set.seed(100)
baseline_prediction = ifelse(mean(dataTrain$Mortality) > 0.5, 1, 0)
baseline_predictions = rep(baseline_prediction, nrow(dataTest))
baseline_accuracy = mean(baseline_predictions == dataTest$Mortality) * 100
baseline_accuracy = round(baseline_accuracy, 2)
str(baseline_accuracy)
data = read.csv("Assignment_Logistic_Regression_Data.csv")
#str(data)
library(caTools)
set.seed(100)
set.seed(100)
split = sample.split(data$Mortality, SplitRatio = 0.7)
dataTrain = subset(data, split == TRUE)
dataTest = subset(data, split == FALSE)
train_observation_num = nrow(dataTrain)
str(train_observation_num)
mort_rate = sum(dataTrain$Mortality) / train_observation_num
str(mort_rate)
set.seed(100)
model2a = glm(Mortality ~ . , data = dataTrain, family = binomial)
summary(model2a)
set.seed(100)
model2c = glm(Mortality ~ SystolicBP + DiastolicBP + SpO2 + Haemoglobin + NumWBC + Urea , data = dataTrain, family = binomial)
summary(model2c)
model3a = glm(Mortality ~ SystolicBP + SpO2 + Haemoglobin + Urea , data = dataTrain, family = binomial)
cor(dataTrain[, c("SystolicBP", "SpO2", "Haemoglobin", "Urea")])
predict3b = predict(model3a, newdata = testData3b, type="response")
knitr::opts_chunk$set(warning = FALSE,
fig.height = 6,
fig.width = 6,
fig.align = "center")
data = read.csv("Assignment_Logistic_Regression_Data.csv")
#str(data)
library(caTools)
set.seed(100)
set.seed(100)
split = sample.split(data$Mortality, SplitRatio = 0.7)
dataTrain = subset(data, split == TRUE)
dataTest = subset(data, split == FALSE)
train_observation_num = nrow(dataTrain)
str(train_observation_num)
mort_rate = sum(dataTrain$Mortality) / train_observation_num
str(mort_rate)
set.seed(100)
model2a = glm(Mortality ~ . , data = dataTrain, family = binomial)
summary(model2a)
set.seed(100)
model2c = glm(Mortality ~ SystolicBP + DiastolicBP + SpO2 + Haemoglobin + NumWBC + Urea , data = dataTrain, family = binomial)
summary(model2c)
set.seed(100)
model3a = glm(Mortality ~ SystolicBP + SpO2 + Haemoglobin + Urea , data = dataTrain, family = binomial)
predictTrain = predict(model3a, type = "response")
cor(dataTrain[, c("SystolicBP", "SpO2", "Haemoglobin", "Urea")])
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
str(res3b)
library(ROCR)
set.seed(100)
predictTest = predict(model3a, newdata = dataTest, type = "response")
binary_predictions = ifelse(predictTest >= 0.1, 1, 0)
accuracy = mean(binary_predictions == dataTest$Mortality) * 100
accuracy = round(accuracy, 2)
str(accuracy)
cof1 = table(dataTest$Mortality, predictTest >0.1 )
cof3 = table(dataTest$Mortality, predictTest >0.3 )
cof5 = table(dataTest$Mortality, predictTest >0.5 )
cof7 = table(dataTest$Mortality, predictTest >0.7 )
cof1
cof3
cof5
cof7
set.seed(100)
baseline_prediction = ifelse(mean(dataTrain$Mortality) > 0.5, 1, 0)
baseline_predictions = rep(baseline_prediction, nrow(dataTest))
baseline_accuracy = mean(baseline_predictions == dataTest$Mortality) * 100
baseline_accuracy = round(baseline_accuracy, 2)
str(baseline_accuracy)
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
str(1-res3b)
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
str(100-res3b)
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(100-predict3b*100, 2)
str(res3b)
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
a = 100-res3b
a
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
a = 100-res3b
str(a)
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
a = 100-res3b
str(a, 2)
set.seed(100)
testData3b = data.frame(SystolicBP = 95, SpO2 = 100, Haemoglobin = 8.3, Urea = 6.5)
predict3b = predict(model3a, newdata = testData3b, type="response")
res3b = round(predict3b*100, 2)
100-res3b
set.seed(100)
predictTest = predict(model3a, newdata = dataTest, type = "response")
binary_predictions = ifelse(predictTest >= 0.1, 1, 0)
accuracy = mean(binary_predictions == dataTest$Mortality) * 100
accuracy = round(accuracy, 2)
accuracy
set.seed(100)
baseline_prediction = ifelse(mean(dataTrain$Mortality) > 0.5, 1, 0)
baseline_predictions = rep(baseline_prediction, nrow(dataTest))
baseline_accuracy = mean(baseline_predictions == dataTest$Mortality) * 100
baseline_accuracy = round(baseline_accuracy, 2)
baseline_accuracy
set.seed(100)
split = sample.split(data$Mortality, SplitRatio = 0.7)
dataTrain = subset(data, split == TRUE)
dataTest = subset(data, split == FALSE)
train_observation_num = nrow(dataTrain)
str(train_observation_num)
mort_rate = sum(dataTrain$Mortality) / train_observation_num
mort_rate
base_cof = table(dataTest$Mortality, baseline_predictions >0.5 )
base_cof = table(dataTest$Mortality, baseline_predictions >0.5 )
base_cof
base_cof = table(dataTest$Mortality, baseline_prediction >0.5 )
base_cof = table(dataTest$Mortality, baseline_predictions >0.5 )
base_cof
base_cof = table(dataTest$Mortality, baseline_predictions >0.1 )
base_cof
set.seed(100)
baseline_prediction = ifelse(mean(dataTrain$Mortality) > 0.5, 1, 0)
baseline_predictions = rep(baseline_prediction, nrow(dataTest))
baseline_predictions
baseline_accuracy = mean(baseline_predictions == dataTest$Mortality) * 100
baseline_accuracy = round(baseline_accuracy, 2)
baseline_accuracy
knitr::opts_chunk$set(warning = FALSE,
fig.height = 6,
fig.width = 6,
fig.align = "center")
train = read.csv("Competition_Train.csv")
test = read.csv("Competition_Test.csv")
str(train)
str(test)
train$potential_issue = as.factor(train$potential_issue)
train[18:23] = lapply(train[18:23], as.factor)
str(train)
test$potential_issue = as.factor(test$potential_issue)
test[18:22] = lapply(test[18:22], as.factor)
str(test)
library(plyr)
train$potential_issue = revalue(train$potential_issue, c("0"="No", "1"="Yes"))
train[18:23] = lapply(train[18:23], function(x) revalue(x, c("0"="No", "1"="Yes")))
str(train)
test$potential_issue = revalue(test$potential_issue, c("0"="No", "1"="Yes"))
test[18:22] = lapply(test[18:22], function(x) revalue(x, c("0"="No", "1"="Yes")))
str(test)
library(caret)
library(e1071)
fitControl = trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
set.seed(123)
train(went_on_backorder ~ . - sku, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
set.seed(123)
train(went_on_backorder ~ national_inv, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
cor(train)
# check correlation
temp_train = train
numeric_col = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
# check correlation
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
str(dataTrain_numeric)
# check correlation
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
str(temp_train)
cor(train)
# check correlation
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
str(temp_train)
cor(temp_train)
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
str(temp_train)
correlation_matrix = cor(temp_train)
corrplot::corrplot(correlation_matrix)
library(mlbench)
library(corrplot)
install.packages("corrplot")
library(corrplot)
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
str(temp_train)
cor(temp_train)
# Load required libraries
library(caret)
library(mlbench)
# Load required libraries
library(caret)
install.packages("mlbench")
library(mlbench)
# Load your dataset
data <- train
# Create a control function for RFE
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
# Perform recursive feature elimination (RFE)
result <- rfe(data[, -which(names(data) == "went_on_backorder")],
data$went_on_backorder,
sizes = c(1:ncol(data)-1),
rfeControl = control)
# Load required libraries
library(caret)
install.packages("mlbench")
install.packages("randomForest")
library(mlbench)
# Load your dataset
data <- train
# Create a control function for RFE
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
# Perform recursive feature elimination (RFE)
result <- rfe(data[, -which(names(data) == "went_on_backorder")],
data$went_on_backorder,
sizes = c(1:ncol(data)-1),
rfeControl = control)
# Load required libraries
library(caret)
library(mlbench)
# Load your dataset
data <- train
# Create a control function for RFE
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
# Perform recursive feature elimination (RFE)
result <- rfe(data[, -which(names(data) == "went_on_backorder")],
data$went_on_backorder,
sizes = c(1:ncol(data)-1),
rfeControl = control)
knitr::opts_chunk$set(warning = FALSE,
fig.height = 6,
fig.width = 6,
fig.align = "center")
train = read.csv("Competition_Train.csv")
test = read.csv("Competition_Test.csv")
str(train)
str(test)
train$potential_issue = as.factor(train$potential_issue)
train[18:23] = lapply(train[18:23], as.factor)
str(train)
test$potential_issue = as.factor(test$potential_issue)
test[18:22] = lapply(test[18:22], as.factor)
str(test)
library(plyr)
train$potential_issue = revalue(train$potential_issue, c("0"="No", "1"="Yes"))
train[18:23] = lapply(train[18:23], function(x) revalue(x, c("0"="No", "1"="Yes")))
str(train)
test$potential_issue = revalue(test$potential_issue, c("0"="No", "1"="Yes"))
test[18:22] = lapply(test[18:22], function(x) revalue(x, c("0"="No", "1"="Yes")))
str(test)
library(caret)
library(e1071)
fitControl = trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
set.seed(123)
train(went_on_backorder ~ . - sku, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
set.seed(123)
train(went_on_backorder ~ national_inv, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
str(temp_train)
cor(temp_train)
install.packages("mlbench")
install.packages("randomForest")
install.packages("mlbench")
install.packages("randomForest")
str(train)
# Load required libraries
library(caret)
library(mlbench)
# Load your dataset
data <- train
# Create a control function for RFE
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
# Perform recursive feature elimination (RFE)
result <- rfe(data[, -which(names(data) == "went_on_backorder")],
data$went_on_backorder,
sizes = c(1:ncol(data)-1),
rfeControl = control)
knitr::opts_chunk$set(warning = FALSE,
fig.height = 6,
fig.width = 6,
fig.align = "center")
train = read.csv("Competition_Train.csv")
test = read.csv("Competition_Test.csv")
#str(train)
#str(test)
train$potential_issue = as.factor(train$potential_issue)
train[18:23] = lapply(train[18:23], as.factor)
#str(train)
test$potential_issue = as.factor(test$potential_issue)
test[18:22] = lapply(test[18:22], as.factor)
#str(test)
library(plyr)
train$potential_issue = revalue(train$potential_issue, c("0"="No", "1"="Yes"))
train[18:23] = lapply(train[18:23], function(x) revalue(x, c("0"="No", "1"="Yes")))
#str(train)
test$potential_issue = revalue(test$potential_issue, c("0"="No", "1"="Yes"))
test[18:22] = lapply(test[18:22], function(x) revalue(x, c("0"="No", "1"="Yes")))
#str(test)
library(caret)
library(e1071)
fitControl = trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
set.seed(123)
#train(went_on_backorder ~ . - sku, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
set.seed(123)
#train(went_on_backorder ~ national_inv, data = train, method = "glm", family = "binomial", trControl = fitControl, metric = "ROC")
temp_train = train
numeric_cols = sapply(temp_train, is.numeric)
# Convert non-numeric columns to numeric
temp_train[!numeric_cols] <- lapply(temp_train[!numeric_cols], as.numeric)
#str(temp_train)
#cor(temp_train)
# Load required libraries
library(caret)
library(mlbench)
# Load your dataset
data <- train
# Create a control function for RFE
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
# Perform recursive feature elimination (RFE)
result <- rfe(data[, -which(names(data) == "went_on_backorder")],
data$went_on_backorder,
sizes = c(1:ncol(data)-1),
rfeControl = control)
# Load required libraries
library(caret)
library(mlbench)
# Load your dataset
data <- train
# Create a control function for RFE
control <- rfeControl(functions = rfFuncs, method = "cv", number = 10)
# Perform recursive feature elimination (RFE)
result <- rfe(data[, -which(names(data) == "went_on_backorder")],
data$went_on_backorder,
sizes = c(1:ncol(data)-1),
rfeControl = control)
